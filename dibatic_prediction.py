# -*- coding: utf-8 -*-
"""Dibatic_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AwS4H1bI_RknpfcaTOpmUXwL-v2eNkJz
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import warnings
warnings.filterwarnings('ignore')

train_df=pd.read_csv('train.csv')
test_df=pd.read_csv('test.csv')

train_df.head()

test_df.head()

train_df.info()

test_df.info()

train_df.describe()

test_df.describe()

train_df.shape

test_df.shape

import seaborn as sns
import matplotlib.pyplot as plt

# Histogram
sns.histplot(train_df["age"], kde=True)
plt.show()

# Boxplot vs target
sns.boxplot(x="diagnosed_diabetes", y="bmi", data=train_df)
plt.show()

# Countplot for categorical
sns.countplot(x="education_level", data=train_df)
plt.show()

train_df.isnull().sum()

test_df.isnull().sum()

train_df.drop(columns=["id"], inplace=True)
test_df.drop(columns=["id"], inplace=True)
train_df.dropna(inplace=True)
test_df.dropna(inplace=True)

from sklearn.preprocessing import LabelEncoder

cat_cols = [
    "gender",
    "smoking_status",
    "employment_status",
    "ethnicity",
    "income_level",
    "education_level"
]
encoders = {}

for col in cat_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col])
    encoders[col] = le

from sklearn.preprocessing import LabelEncoder

cat_cols = [
    "gender",
    "smoking_status",
    "employment_status",
    "ethnicity",
    "income_level",
    "education_level"
]
encoders = {}

for col in cat_cols:
    le = LabelEncoder()
    test_df[col] = le.fit_transform(test_df[col])
    encoders[col] = le

test_df.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
corr = train_df.corr(method='pearson')

plt.figure(figsize=(10,8))
sns.heatmap(corr, annot=False, cmap='coolwarm', linewidths=0.5)
plt.title('Pearson Correlation Heatmap')
plt.show()

high_corr = corr.unstack().sort_values(ascending=False)
high_corr = high_corr[(high_corr < 1) & (high_corr > 0.7)]
print(high_corr)

X=train_df.drop(columns=["diagnosed_diabetes"])
y=train_df["diagnosed_diabetes"]

from sklearn.preprocessing import StandardScaler
columns_to_scale = X.select_dtypes(include=["int64", "float64"]).columns
scaler = StandardScaler()
X[columns_to_scale] = scaler.fit_transform(X[columns_to_scale])

test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from lightgbm import LGBMClassifier

model = LGBMClassifier(
    # Core
    boosting_type="gbdt",
    objective="binary",
    metric="auc",
    class_weight="balanced",

    # Trees
    n_estimators=500,
    learning_rate=0.05,
    num_leaves=31,
    max_depth=-1,
    min_child_samples=20,

    # Sampling
    subsample=0.8,              # aka bagging_fraction
    subsample_freq=1,
    colsample_bytree=0.8,       # aka feature_fraction

    # Regularization
    reg_alpha=0.1,              # L1
    reg_lambda=0.1,             # L2

    # Other
    random_state=42,
    n_jobs=-1,
    importance_type="gain"
)

model.fit(X_train, y_train)

from sklearn.metrics import roc_auc_score

# Predict probabilities on the validation set (X_val)
val_pred = model.predict_proba(X_val)[:, 1]

# Calculate AUC score using y_val and predictions on X_val
auc = roc_auc_score(y_val, val_pred)
print("Validation AUC:", auc)

from sklearn.metrics import classification_report

val_pred = (val_pred >= 0.5).astype(int)

from sklearn.metrics import classification_report

print(classification_report(y_val, val_pred))

test_df_orig = pd.read_csv('test.csv')
test_df_orig.head()

"""**Reasoning**:
I need to remove the 'id' column from `test_df_orig` as it is not a feature for model training and then handle any missing values to ensure the dataset is clean before applying transformations.


"""

test_df_orig.drop(columns=["id"], inplace=True)
test_df_orig.dropna(inplace=True)
test_df_orig.head()

for col, encoder in encoders.items():
    if col in test_df_orig.columns:
        test_df_orig[col] = encoder.transform(test_df_orig[col])
test_df_orig.head()

test_df_orig[columns_to_scale] = scaler.transform(test_df_orig[columns_to_scale])
test_df_orig.head()

test_predictions_proba = model.predict_proba(test_df_orig)[:, 1]
test_predictions_proba[:5]

test_predictions = (test_predictions_proba >= 0.5).astype(int)
test_predictions[:5]

original_test_df = pd.read_csv('test.csv')
submission_ids = original_test_df.dropna(subset=test_df_orig.columns).id

submission_df = pd.DataFrame({'id': submission_ids, 'diagnosed_diabetes': test_predictions})
submission_df.head()

submission_df.to_csv('submission.csv', index=False)
print("Submission file 'submission.csv' created successfully.")

import pickle

# Save the trained model to a .pkl file
with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)

print("Model saved successfully as model.pkl")